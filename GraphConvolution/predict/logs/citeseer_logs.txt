----------------------------------------------------------
Time: 0516-1550
Training Loss: 
0.6716, 0.5559, 0.4871, 0.4733, 0.4439, 0.4407, 0.4485, 0.4410, 0.4350, 0.4361, **Epoch: 10
0.4331, 0.4377, 0.4295, 0.4282, 0.4266, 0.4241, 0.4286, 0.4250, 0.4216, 0.4256, **Epoch: 20
0.4297, 0.4280, 0.4254, 0.4229, 0.4166, 0.4206, 0.4211, 0.4218, 0.4186, 0.4180, **Epoch: 30
0.4171, 0.4136, 0.4150, 0.4133, 0.4129, 0.4129, 0.4168, 0.4138, 0.4121, 0.4165, **Epoch: 40
0.4095, 0.4181, 0.4146, 0.4108, 0.4203, 0.4181, 0.4137, 0.4105, 0.4060, 0.4154, **Epoch: 50
0.4122, 0.4154, 0.4153, 0.4128, 0.4108, 0.4107, 0.4152, 0.4071, 0.4089, 0.4107, **Epoch: 60
0.4090, 0.4104, 0.4058, 0.4145, 0.4144, 0.4137, 0.4113, 0.4085, 0.4135, 0.4125, **Epoch: 70
0.4037, 0.4103, 0.4071, 0.4112, 0.4086, 0.4085, 0.4064, 0.4121, 0.4104, 0.4079, **Epoch: 80
0.4130, 0.4082, 0.4083, 0.4093, 0.4088, 0.4058, 0.4087, 0.4053, 0.4058, 0.3994, **Epoch: 90
0.4054, 0.4030, 0.4085, 0.4073, 0.4053, 0.4089, 0.4071, 0.4087, 0.4022, 0.4025, **Epoch: 100
Validation Loss: 
0.5846, 0.5137, 0.4959, 0.4713, 0.4784, 0.4817, 0.4727, 0.4680, 0.4734, 0.4709, **Epoch: 10
0.4688, 0.4734, 0.4673, 0.4737, 0.4669, 0.4706, 0.4593, 0.4716, 0.4736, 0.4581, **Epoch: 20
0.4602, 0.4584, 0.4622, 0.4648, 0.4660, 0.4633, 0.4597, 0.4483, 0.4632, 0.4615, **Epoch: 30
0.4568, 0.4569, 0.4596, 0.4539, 0.4610, 0.4558, 0.4537, 0.4548, 0.4526, 0.4537, **Epoch: 40
0.4459, 0.4593, 0.4587, 0.4555, 0.4570, 0.4587, 0.4622, 0.4549, 0.4562, 0.4584, **Epoch: 50
0.4540, 0.4562, 0.4576, 0.4500, 0.4567, 0.4591, 0.4609, 0.4511, 0.4588, 0.4550, **Epoch: 60
0.4519, 0.4505, 0.4553, 0.4574, 0.4543, 0.4522, 0.4530, 0.4491, 0.4530, 0.4522, **Epoch: 70
0.4541, 0.4553, 0.4533, 0.4484, 0.4454, 0.4466, 0.4527, 0.4468, 0.4526, 0.4566, **Epoch: 80
0.4516, 0.4528, 0.4488, 0.4579, 0.4516, 0.4517, 0.4500, 0.4603, 0.4520, 0.4564, **Epoch: 90
0.4477, 0.4510, 0.4478, 0.4507, 0.4493, 0.4529, 0.4522, 0.4484, 0.4531, 0.4551, **Epoch: 100
Training AUC: 
0.6910, 0.9594, 0.9493, 0.9528, 0.9753, 0.9810, 0.9820, 0.9830, 0.9866, 0.9869, **Epoch: 10
0.9886, 0.9879, 0.9902, 0.9898, 0.9910, 0.9913, 0.9906, 0.9907, 0.9931, 0.9912, **Epoch: 20
0.9927, 0.9906, 0.9923, 0.9938, 0.9942, 0.9946, 0.9949, 0.9936, 0.9931, 0.9922, **Epoch: 30
0.9946, 0.9948, 0.9947, 0.9951, 0.9944, 0.9940, 0.9937, 0.9945, 0.9940, 0.9938, **Epoch: 40
0.9946, 0.9931, 0.9944, 0.9948, 0.9945, 0.9937, 0.9943, 0.9948, 0.9963, 0.9946, **Epoch: 50
0.9951, 0.9945, 0.9940, 0.9955, 0.9940, 0.9944, 0.9943, 0.9960, 0.9959, 0.9955, **Epoch: 60
0.9960, 0.9955, 0.9972, 0.9944, 0.9940, 0.9954, 0.9963, 0.9963, 0.9951, 0.9952, **Epoch: 70
0.9966, 0.9961, 0.9960, 0.9953, 0.9961, 0.9958, 0.9963, 0.9960, 0.9960, 0.9956, **Epoch: 80
0.9958, 0.9968, 0.9953, 0.9956, 0.9965, 0.9959, 0.9955, 0.9968, 0.9950, 0.9978, **Epoch: 90
0.9958, 0.9967, 0.9960, 0.9963, 0.9966, 0.9962, 0.9961, 0.9953, 0.9966, 0.9960, **Epoch: 100
Validation AUC: 
0.9046, 0.9087, 0.9203, 0.9404, 0.9405, 0.9380, 0.9420, 0.9447, 0.9432, 0.9449, **Epoch: 10
0.9449, 0.9419, 0.9450, 0.9433, 0.9454, 0.9437, 0.9488, 0.9447, 0.9434, 0.9511, **Epoch: 20
0.9496, 0.9515, 0.9499, 0.9485, 0.9485, 0.9493, 0.9500, 0.9542, 0.9498, 0.9499, **Epoch: 30
0.9523, 0.9514, 0.9511, 0.9515, 0.9493, 0.9512, 0.9508, 0.9507, 0.9512, 0.9505, **Epoch: 40
0.9538, 0.9494, 0.9494, 0.9512, 0.9500, 0.9490, 0.9478, 0.9508, 0.9493, 0.9489, **Epoch: 50
0.9513, 0.9505, 0.9491, 0.9527, 0.9508, 0.9507, 0.9494, 0.9533, 0.9503, 0.9524, **Epoch: 60
0.9537, 0.9537, 0.9520, 0.9511, 0.9524, 0.9530, 0.9536, 0.9560, 0.9532, 0.9541, **Epoch: 70
0.9542, 0.9536, 0.9536, 0.9547, 0.9566, 0.9552, 0.9530, 0.9560, 0.9535, 0.9524, **Epoch: 80
0.9543, 0.9545, 0.9561, 0.9516, 0.9533, 0.9534, 0.9556, 0.9513, 0.9544, 0.9519, **Epoch: 90
0.9560, 0.9542, 0.9561, 0.9551, 0.9551, 0.9537, 0.9539, 0.9550, 0.9532, 0.9524, **Epoch: 100
Validation AUC Final: 0.9524
Learning rate: 0.01
Weight decay: 0.0005
Drop rate: 0
Epochs: 100
--------------------------END------------------------------
----------------------------------------------------------
Time: 0516-1550
Training Loss: 
0.6748, 0.5865, 0.5230, 0.5184, 0.5036, 0.5000, 0.4988, 0.4965, 0.4853, 0.4878, **Epoch: 10
0.4875, 0.4860, 0.4940, 0.4816, 0.4803, 0.4760, 0.4887, 0.4769, 0.4776, 0.4717, **Epoch: 20
0.4784, 0.4687, 0.4733, 0.4737, 0.4721, 0.4678, 0.4640, 0.4788, 0.4744, 0.4707, **Epoch: 30
0.4619, 0.4790, 0.4679, 0.4656, 0.4617, 0.4763, 0.4684, 0.4591, 0.4662, 0.4662, **Epoch: 40
0.4636, 0.4553, 0.4660, 0.4591, 0.4647, 0.4579, 0.4593, 0.4634, 0.4634, 0.4617, **Epoch: 50
0.4577, 0.4794, 0.4728, 0.4628, 0.4690, 0.4589, 0.4635, 0.4617, 0.4586, 0.4631, **Epoch: 60
0.4669, 0.4547, 0.4645, 0.4640, 0.4534, 0.4568, 0.4612, 0.4648, 0.4679, 0.4605, **Epoch: 70
0.4633, 0.4582, 0.4580, 0.4610, 0.4599, 0.4567, 0.4598, 0.4585, 0.4572, 0.4553, **Epoch: 80
0.4619, 0.4519, 0.4567, 0.4603, 0.4537, 0.4605, 0.4629, 0.4579, 0.4589, 0.4515, **Epoch: 90
0.4614, 0.4588, 0.4557, 0.4585, 0.4548, 0.4565, 0.4628, 0.4611, 0.4645, 0.4540, **Epoch: 100
Validation Loss: 
0.6074, 0.5473, 0.5316, 0.5205, 0.5242, 0.5254, 0.5169, 0.5151, 0.5113, 0.5091, **Epoch: 10
0.5065, 0.5161, 0.5173, 0.5169, 0.5113, 0.5056, 0.5054, 0.5152, 0.5063, 0.5185, **Epoch: 20
0.5091, 0.5056, 0.5061, 0.5133, 0.5075, 0.5089, 0.5087, 0.5035, 0.5153, 0.5034, **Epoch: 30
0.5029, 0.5052, 0.5016, 0.4973, 0.5006, 0.5022, 0.4910, 0.5005, 0.4979, 0.4970, **Epoch: 40
0.5043, 0.4955, 0.5002, 0.4941, 0.5040, 0.4994, 0.4936, 0.5019, 0.5021, 0.4984, **Epoch: 50
0.4918, 0.4905, 0.4931, 0.4919, 0.4973, 0.4907, 0.5039, 0.4995, 0.4985, 0.4983, **Epoch: 60
0.5010, 0.4966, 0.5044, 0.4986, 0.5075, 0.4955, 0.4931, 0.4977, 0.4933, 0.4901, **Epoch: 70
0.4998, 0.4889, 0.5016, 0.4913, 0.5003, 0.4918, 0.4909, 0.4988, 0.4952, 0.4964, **Epoch: 80
0.4953, 0.5011, 0.4917, 0.4983, 0.4948, 0.4916, 0.4962, 0.4914, 0.4998, 0.4880, **Epoch: 90
0.4977, 0.4918, 0.4890, 0.4944, 0.4902, 0.4983, 0.5006, 0.4973, 0.4884, 0.4902, **Epoch: 100
Training AUC: 
0.6657, 0.8700, 0.8886, 0.8781, 0.8932, 0.8938, 0.8942, 0.8988, 0.9081, 0.9037, **Epoch: 10
0.9060, 0.9043, 0.8980, 0.9098, 0.9054, 0.9134, 0.9025, 0.9133, 0.9120, 0.9151, **Epoch: 20
0.9116, 0.9194, 0.9153, 0.9170, 0.9171, 0.9232, 0.9209, 0.9081, 0.9152, 0.9184, **Epoch: 30
0.9254, 0.9057, 0.9202, 0.9227, 0.9258, 0.9143, 0.9114, 0.9275, 0.9228, 0.9270, **Epoch: 40
0.9253, 0.9304, 0.9272, 0.9287, 0.9213, 0.9256, 0.9295, 0.9252, 0.9250, 0.9222, **Epoch: 50
0.9296, 0.9034, 0.9156, 0.9240, 0.9179, 0.9252, 0.9194, 0.9243, 0.9303, 0.9233, **Epoch: 60
0.9233, 0.9272, 0.9222, 0.9208, 0.9323, 0.9254, 0.9220, 0.9168, 0.9208, 0.9230, **Epoch: 70
0.9259, 0.9244, 0.9245, 0.9291, 0.9254, 0.9297, 0.9235, 0.9274, 0.9279, 0.9316, **Epoch: 80
0.9242, 0.9326, 0.9261, 0.9237, 0.9295, 0.9255, 0.9174, 0.9254, 0.9231, 0.9318, **Epoch: 90
0.9230, 0.9250, 0.9292, 0.9276, 0.9335, 0.9254, 0.9212, 0.9226, 0.9197, 0.9305, **Epoch: 100
Validation AUC: 
0.8249, 0.8515, 0.8562, 0.8702, 0.8631, 0.8617, 0.8735, 0.8739, 0.8779, 0.8814, **Epoch: 10
0.8845, 0.8713, 0.8720, 0.8720, 0.8722, 0.8825, 0.8788, 0.8716, 0.8764, 0.8633, **Epoch: 20
0.8792, 0.8828, 0.8797, 0.8722, 0.8798, 0.8780, 0.8766, 0.8814, 0.8713, 0.8850, **Epoch: 30
0.8850, 0.8842, 0.8888, 0.8906, 0.8852, 0.8866, 0.8954, 0.8928, 0.8945, 0.8902, **Epoch: 40
0.8823, 0.8926, 0.8882, 0.8925, 0.8767, 0.8872, 0.8937, 0.8874, 0.8838, 0.8874, **Epoch: 50
0.8983, 0.8874, 0.8905, 0.8917, 0.8921, 0.8960, 0.8809, 0.8884, 0.8882, 0.8886, **Epoch: 60
0.8858, 0.8872, 0.8867, 0.8890, 0.8795, 0.8847, 0.8932, 0.8906, 0.8860, 0.8983, **Epoch: 70
0.8852, 0.8963, 0.8826, 0.8924, 0.8853, 0.8920, 0.8962, 0.8904, 0.8894, 0.8907, **Epoch: 80
0.8945, 0.8896, 0.8912, 0.8859, 0.8884, 0.8936, 0.8888, 0.8907, 0.8869, 0.8974, **Epoch: 90
0.8892, 0.8948, 0.8930, 0.8914, 0.8988, 0.8902, 0.8836, 0.8906, 0.8956, 0.8953, **Epoch: 100
Validation AUC Final: 0.8953
Learning rate: 0.01
Weight decay: 0.0005
Drop rate: 0.05
Epochs: 100
--------------------------END------------------------------
----------------------------------------------------------
Time: 0516-1552
Training Loss: 
0.6750, 0.5868, 0.5355, 0.5120, 0.4966, 0.5044, 0.4839, 0.4858, 0.4854, 0.4913, **Epoch: 10
0.4812, 0.4838, 0.4812, 0.4824, 0.4799, 0.4777, 0.4758, 0.4721, 0.4780, 0.4695, **Epoch: 20
0.4725, 0.4794, 0.4742, 0.4706, 0.4749, 0.4795, 0.4786, 0.4648, 0.4730, 0.4672, **Epoch: 30
0.4693, 0.4709, 0.4721, 0.4724, 0.4585, 0.4763, 0.4692, 0.4657, 0.4687, 0.4637, **Epoch: 40
0.4673, 0.4716, 0.4709, 0.4724, 0.4707, 0.4609, 0.4551, 0.4603, 0.4637, 0.4572, **Epoch: 50
0.4696, 0.4711, 0.4651, 0.4603, 0.4632, 0.4536, 0.4642, 0.4621, 0.4605, 0.4647, **Epoch: 60
0.4656, 0.4541, 0.4647, 0.4536, 0.4593, 0.4604, 0.4613, 0.4589, 0.4660, 0.4582, **Epoch: 70
0.4598, 0.4654, 0.4536, 0.4553, 0.4617, 0.4568, 0.4595, 0.4453, 0.4602, 0.4615, **Epoch: 80
0.4543, 0.4594, 0.4689, 0.4584, 0.4605, 0.4655, 0.4653, 0.4547, 0.4710, 0.4638, **Epoch: 90
0.4589, 0.4551, 0.4611, 0.4577, 0.4572, 0.4563, 0.4535, 0.4621, 0.4567, 0.4507, **Epoch: 100
Validation Loss: 
0.6105, 0.5491, 0.5440, 0.5160, 0.5168, 0.5173, 0.5156, 0.5136, 0.5089, 0.5233, **Epoch: 10
0.5026, 0.5207, 0.5125, 0.5046, 0.5109, 0.5082, 0.5146, 0.5042, 0.5122, 0.5129, **Epoch: 20
0.5090, 0.5119, 0.5211, 0.5065, 0.5033, 0.4964, 0.5030, 0.5138, 0.5079, 0.4933, **Epoch: 30
0.5087, 0.5049, 0.5034, 0.4980, 0.5066, 0.5057, 0.4941, 0.5011, 0.5025, 0.5026, **Epoch: 40
0.5075, 0.4937, 0.5055, 0.5052, 0.5020, 0.4972, 0.4968, 0.4992, 0.5007, 0.5010, **Epoch: 50
0.5017, 0.5052, 0.4923, 0.5137, 0.4995, 0.4993, 0.4985, 0.5054, 0.4994, 0.5034, **Epoch: 60
0.4968, 0.5077, 0.5096, 0.4970, 0.5051, 0.4936, 0.4882, 0.4975, 0.5023, 0.4930, **Epoch: 70
0.5017, 0.4989, 0.5023, 0.4943, 0.4908, 0.4937, 0.4976, 0.5017, 0.4885, 0.4985, **Epoch: 80
0.5083, 0.4950, 0.4881, 0.5007, 0.4967, 0.5086, 0.4915, 0.5018, 0.4967, 0.4982, **Epoch: 90
0.4924, 0.4966, 0.4939, 0.4928, 0.4983, 0.4915, 0.5009, 0.4949, 0.4849, 0.4862, **Epoch: 100
Training AUC: 
0.6602, 0.8741, 0.8758, 0.8873, 0.9010, 0.8895, 0.9117, 0.9076, 0.9095, 0.9036, **Epoch: 10
0.9125, 0.9112, 0.9087, 0.9085, 0.9117, 0.9142, 0.9136, 0.9205, 0.9130, 0.9190, **Epoch: 20
0.9215, 0.9084, 0.9193, 0.9195, 0.9122, 0.9094, 0.9104, 0.9246, 0.9180, 0.9207, **Epoch: 30
0.9251, 0.9179, 0.9142, 0.9187, 0.9259, 0.9155, 0.9194, 0.9241, 0.9156, 0.9225, **Epoch: 40
0.9242, 0.9131, 0.9146, 0.9152, 0.9172, 0.9272, 0.9325, 0.9292, 0.9233, 0.9315, **Epoch: 50
0.9145, 0.9161, 0.9238, 0.9233, 0.9210, 0.9305, 0.9187, 0.9200, 0.9262, 0.9243, **Epoch: 60
0.9176, 0.9253, 0.9213, 0.9294, 0.9278, 0.9280, 0.9253, 0.9263, 0.9192, 0.9267, **Epoch: 70
0.9311, 0.9240, 0.9311, 0.9339, 0.9223, 0.9259, 0.9266, 0.9374, 0.9253, 0.9270, **Epoch: 80
0.9332, 0.9265, 0.9171, 0.9280, 0.9250, 0.9147, 0.9164, 0.9291, 0.9192, 0.9194, **Epoch: 90
0.9264, 0.9326, 0.9246, 0.9280, 0.9308, 0.9259, 0.9304, 0.9190, 0.9290, 0.9350, **Epoch: 100
Validation AUC: 
0.8329, 0.8567, 0.8424, 0.8755, 0.8734, 0.8742, 0.8766, 0.8733, 0.8750, 0.8647, **Epoch: 10
0.8827, 0.8660, 0.8738, 0.8811, 0.8696, 0.8766, 0.8780, 0.8814, 0.8758, 0.8713, **Epoch: 20
0.8781, 0.8757, 0.8616, 0.8782, 0.8827, 0.8864, 0.8797, 0.8704, 0.8766, 0.8902, **Epoch: 30
0.8811, 0.8812, 0.8781, 0.8866, 0.8827, 0.8791, 0.8913, 0.8856, 0.8842, 0.8862, **Epoch: 40
0.8791, 0.8963, 0.8812, 0.8764, 0.8836, 0.8864, 0.8857, 0.8900, 0.8842, 0.8847, **Epoch: 50
0.8843, 0.8814, 0.8926, 0.8748, 0.8862, 0.8799, 0.8839, 0.8805, 0.8856, 0.8839, **Epoch: 60
0.8882, 0.8789, 0.8752, 0.8878, 0.8792, 0.8921, 0.8983, 0.8886, 0.8854, 0.8895, **Epoch: 70
0.8863, 0.8864, 0.8777, 0.8896, 0.8951, 0.8891, 0.8889, 0.8879, 0.8967, 0.8872, **Epoch: 80
0.8770, 0.8859, 0.8963, 0.8829, 0.8877, 0.8766, 0.8923, 0.8849, 0.8887, 0.8863, **Epoch: 90
0.8910, 0.8906, 0.8885, 0.8904, 0.8841, 0.8910, 0.8859, 0.8914, 0.8959, 0.9005, **Epoch: 100
Validation AUC Final: 0.9005
Learning rate: 0.01
Weight decay: 0.0005
Drop rate: 0.05
Epochs: 100
--------------------------END------------------------------
----------------------------------------------------------
Time: 0516-1552
Training Loss: 
0.6703, 0.5539, 0.4802, 0.4666, 0.4602, 0.4394, 0.4340, 0.4358, 0.4332, 0.4442, **Epoch: 10
0.4288, 0.4385, 0.4276, 0.4289, 0.4251, 0.4219, 0.4193, 0.4251, 0.4309, 0.4240, **Epoch: 20
0.4247, 0.4254, 0.4179, 0.4187, 0.4178, 0.4257, 0.4272, 0.4234, 0.4246, 0.4236, **Epoch: 30
0.4170, 0.4260, 0.4212, 0.4211, 0.4175, 0.4135, 0.4190, 0.4127, 0.4160, 0.4197, **Epoch: 40
0.4105, 0.4135, 0.4126, 0.4115, 0.4171, 0.4019, 0.4177, 0.4103, 0.4224, 0.4174, **Epoch: 50
0.4163, 0.4113, 0.4115, 0.4141, 0.4133, 0.4135, 0.4063, 0.4135, 0.4077, 0.4110, **Epoch: 60
0.4078, 0.4090, 0.4100, 0.4108, 0.4107, 0.4079, 0.4086, 0.4100, 0.4055, 0.4087, **Epoch: 70
0.4157, 0.4086, 0.4154, 0.4073, 0.4069, 0.4145, 0.4058, 0.4073, 0.4035, 0.4030, **Epoch: 80
0.4096, 0.4081, 0.4032, 0.4038, 0.3996, 0.4091, 0.4024, 0.4092, 0.4043, 0.4029, **Epoch: 90
0.4076, 0.4047, 0.3999, 0.4053, 0.4008, 0.4033, 0.4045, 0.4072, 0.4045, 0.4027, **Epoch: 100
Validation Loss: 
0.5838, 0.5113, 0.4901, 0.4731, 0.4732, 0.4679, 0.4743, 0.4714, 0.4667, 0.4740, **Epoch: 10
0.4797, 0.4711, 0.4626, 0.4758, 0.4620, 0.4671, 0.4689, 0.4637, 0.4642, 0.4728, **Epoch: 20
0.4684, 0.4643, 0.4668, 0.4609, 0.4737, 0.4683, 0.4600, 0.4599, 0.4659, 0.4643, **Epoch: 30
0.4662, 0.4624, 0.4663, 0.4656, 0.4558, 0.4694, 0.4631, 0.4615, 0.4584, 0.4624, **Epoch: 40
0.4600, 0.4558, 0.4570, 0.4598, 0.4620, 0.4594, 0.4565, 0.4624, 0.4573, 0.4564, **Epoch: 50
0.4609, 0.4629, 0.4648, 0.4618, 0.4603, 0.4524, 0.4557, 0.4583, 0.4617, 0.4529, **Epoch: 60
0.4541, 0.4551, 0.4505, 0.4584, 0.4573, 0.4576, 0.4556, 0.4598, 0.4616, 0.4569, **Epoch: 70
0.4568, 0.4511, 0.4531, 0.4520, 0.4574, 0.4542, 0.4518, 0.4606, 0.4565, 0.4560, **Epoch: 80
0.4590, 0.4577, 0.4651, 0.4577, 0.4566, 0.4577, 0.4573, 0.4537, 0.4526, 0.4582, **Epoch: 90
0.4598, 0.4498, 0.4576, 0.4521, 0.4543, 0.4523, 0.4502, 0.4522, 0.4511, 0.4482, **Epoch: 100
Training AUC: 
0.6989, 0.9642, 0.9664, 0.9605, 0.9718, 0.9809, 0.9846, 0.9843, 0.9860, 0.9858, **Epoch: 10
0.9897, 0.9886, 0.9898, 0.9901, 0.9899, 0.9918, 0.9915, 0.9915, 0.9908, 0.9906, **Epoch: 20
0.9921, 0.9935, 0.9931, 0.9918, 0.9914, 0.9906, 0.9907, 0.9908, 0.9914, 0.9918, **Epoch: 30
0.9944, 0.9932, 0.9931, 0.9944, 0.9935, 0.9947, 0.9941, 0.9937, 0.9943, 0.9937, **Epoch: 40
0.9950, 0.9939, 0.9932, 0.9939, 0.9920, 0.9945, 0.9929, 0.9947, 0.9912, 0.9934, **Epoch: 50
0.9923, 0.9940, 0.9953, 0.9950, 0.9956, 0.9940, 0.9960, 0.9955, 0.9957, 0.9947, **Epoch: 60
0.9953, 0.9960, 0.9943, 0.9948, 0.9947, 0.9954, 0.9951, 0.9956, 0.9951, 0.9956, **Epoch: 70
0.9944, 0.9946, 0.9941, 0.9957, 0.9953, 0.9943, 0.9956, 0.9967, 0.9968, 0.9964, **Epoch: 80
0.9952, 0.9953, 0.9965, 0.9951, 0.9968, 0.9941, 0.9972, 0.9957, 0.9958, 0.9960, **Epoch: 90
0.9958, 0.9971, 0.9973, 0.9956, 0.9968, 0.9956, 0.9959, 0.9963, 0.9964, 0.9945, **Epoch: 100
Validation AUC: 
0.9045, 0.9269, 0.9264, 0.9415, 0.9439, 0.9444, 0.9403, 0.9407, 0.9415, 0.9390, **Epoch: 10
0.9348, 0.9406, 0.9443, 0.9388, 0.9447, 0.9423, 0.9431, 0.9453, 0.9452, 0.9425, **Epoch: 20
0.9433, 0.9460, 0.9458, 0.9469, 0.9418, 0.9432, 0.9470, 0.9468, 0.9444, 0.9444, **Epoch: 30
0.9436, 0.9450, 0.9436, 0.9425, 0.9466, 0.9410, 0.9436, 0.9453, 0.9465, 0.9443, **Epoch: 40
0.9460, 0.9468, 0.9455, 0.9449, 0.9433, 0.9438, 0.9453, 0.9422, 0.9436, 0.9450, **Epoch: 50
0.9425, 0.9420, 0.9418, 0.9428, 0.9440, 0.9467, 0.9455, 0.9446, 0.9433, 0.9465, **Epoch: 60
0.9462, 0.9464, 0.9477, 0.9442, 0.9456, 0.9460, 0.9468, 0.9452, 0.9450, 0.9464, **Epoch: 70
0.9466, 0.9494, 0.9481, 0.9490, 0.9475, 0.9478, 0.9487, 0.9460, 0.9472, 0.9469, **Epoch: 80
0.9470, 0.9464, 0.9431, 0.9471, 0.9479, 0.9472, 0.9468, 0.9481, 0.9489, 0.9470, **Epoch: 90
0.9457, 0.9505, 0.9477, 0.9488, 0.9476, 0.9487, 0.9494, 0.9487, 0.9486, 0.9507, **Epoch: 100
Validation AUC Final: 0.9507
Learning rate: 0.01
Weight decay: 0.0005
Drop rate: 0
Epochs: 100
--------------------------END------------------------------
----------------------------------------------------------
Time: 0519-1040
Training Loss: 
0.6724, 0.5637, 0.5001, 0.4741, 0.4525, 0.4465, 0.4447, 0.4438, 0.4439, 0.4293, **Epoch: 10
0.4330, 0.4314, 0.4310, 0.4314, 0.4315, 0.4209, 0.4249, 0.4232, 0.4211, 0.4230, **Epoch: 20
0.4164, 0.4238, 0.4257, 0.4198, 0.4224, 0.4193, 0.4216, 0.4269, 0.4255, 0.4175, **Epoch: 30
0.4232, 0.4182, 0.4126, 0.4157, 0.4157, 0.4194, 0.4152, 0.4166, 0.4168, 0.4143, **Epoch: 40
0.4102, 0.4175, 0.4134, 0.4107, 0.4175, 0.4118, 0.4109, 0.4137, 0.4140, 0.4069, **Epoch: 50
0.4084, 0.4107, 0.4074, 0.4065, 0.4113, 0.4143, 0.4082, 0.4133, 0.4103, 0.4041, **Epoch: 60
0.4107, 0.4062, 0.3999, 0.4075, 0.4136, 0.4114, 0.4085, 0.4033, 0.4043, 0.4032, **Epoch: 70
0.4032, 0.4086, 0.4075, 0.4140, 0.4056, 0.4123, 0.4044, 0.4038, 0.4090, 0.4073, **Epoch: 80
0.4022, 0.4110, 0.4061, 0.4056, 0.4044, 0.4013, 0.4031, 0.4025, 0.4092, 0.4065, **Epoch: 90
0.4058, 0.4025, 0.4058, 0.4107, 0.4030, 0.4042, 0.4061, 0.4031, 0.3955, 0.4131, **Epoch: 100
Validation Loss: 
0.5907, 0.5200, 0.4952, 0.4841, 0.4727, 0.4747, 0.4706, 0.4798, 0.4682, 0.4708, **Epoch: 10
0.4656, 0.4699, 0.4712, 0.4637, 0.4639, 0.4632, 0.4570, 0.4622, 0.4645, 0.4632, **Epoch: 20
0.4648, 0.4631, 0.4576, 0.4618, 0.4651, 0.4649, 0.4611, 0.4628, 0.4609, 0.4638, **Epoch: 30
0.4666, 0.4558, 0.4600, 0.4624, 0.4648, 0.4699, 0.4658, 0.4577, 0.4628, 0.4621, **Epoch: 40
0.4598, 0.4597, 0.4610, 0.4575, 0.4617, 0.4586, 0.4605, 0.4533, 0.4611, 0.4564, **Epoch: 50
0.4561, 0.4552, 0.4467, 0.4529, 0.4586, 0.4584, 0.4553, 0.4591, 0.4556, 0.4518, **Epoch: 60
0.4567, 0.4513, 0.4579, 0.4614, 0.4520, 0.4545, 0.4525, 0.4505, 0.4494, 0.4458, **Epoch: 70
0.4543, 0.4510, 0.4488, 0.4535, 0.4533, 0.4656, 0.4488, 0.4552, 0.4533, 0.4542, **Epoch: 80
0.4493, 0.4550, 0.4515, 0.4453, 0.4530, 0.4476, 0.4502, 0.4517, 0.4578, 0.4471, **Epoch: 90
0.4438, 0.4534, 0.4544, 0.4405, 0.4507, 0.4458, 0.4549, 0.4518, 0.4502, 0.4476, **Epoch: 100
Training AUC: 
0.6854, 0.9553, 0.9426, 0.9558, 0.9723, 0.9775, 0.9769, 0.9813, 0.9843, 0.9863, **Epoch: 10
0.9876, 0.9899, 0.9910, 0.9907, 0.9902, 0.9913, 0.9928, 0.9912, 0.9930, 0.9930, **Epoch: 20
0.9941, 0.9932, 0.9918, 0.9918, 0.9934, 0.9922, 0.9928, 0.9938, 0.9920, 0.9950, **Epoch: 30
0.9928, 0.9934, 0.9943, 0.9931, 0.9949, 0.9931, 0.9934, 0.9941, 0.9949, 0.9939, **Epoch: 40
0.9938, 0.9927, 0.9945, 0.9954, 0.9948, 0.9947, 0.9948, 0.9966, 0.9952, 0.9954, **Epoch: 50
0.9955, 0.9948, 0.9956, 0.9959, 0.9952, 0.9938, 0.9959, 0.9941, 0.9956, 0.9969, **Epoch: 60
0.9952, 0.9956, 0.9957, 0.9967, 0.9956, 0.9944, 0.9960, 0.9960, 0.9974, 0.9963, **Epoch: 70
0.9959, 0.9960, 0.9964, 0.9947, 0.9957, 0.9956, 0.9965, 0.9964, 0.9962, 0.9953, **Epoch: 80
0.9955, 0.9956, 0.9965, 0.9972, 0.9965, 0.9964, 0.9964, 0.9969, 0.9951, 0.9968, **Epoch: 90
0.9960, 0.9964, 0.9968, 0.9959, 0.9962, 0.9957, 0.9963, 0.9959, 0.9969, 0.9964, **Epoch: 100
Validation AUC: 
0.8946, 0.9077, 0.9213, 0.9342, 0.9392, 0.9388, 0.9396, 0.9375, 0.9446, 0.9444, **Epoch: 10
0.9461, 0.9454, 0.9452, 0.9471, 0.9471, 0.9484, 0.9487, 0.9461, 0.9462, 0.9468, **Epoch: 20
0.9448, 0.9454, 0.9477, 0.9466, 0.9454, 0.9455, 0.9459, 0.9455, 0.9466, 0.9450, **Epoch: 30
0.9424, 0.9467, 0.9466, 0.9449, 0.9443, 0.9419, 0.9428, 0.9477, 0.9446, 0.9454, **Epoch: 40
0.9457, 0.9466, 0.9455, 0.9481, 0.9466, 0.9469, 0.9458, 0.9489, 0.9475, 0.9486, **Epoch: 50
0.9490, 0.9494, 0.9522, 0.9499, 0.9494, 0.9486, 0.9501, 0.9482, 0.9507, 0.9518, **Epoch: 60
0.9498, 0.9522, 0.9498, 0.9492, 0.9511, 0.9511, 0.9510, 0.9522, 0.9536, 0.9551, **Epoch: 70
0.9516, 0.9533, 0.9530, 0.9520, 0.9512, 0.9472, 0.9534, 0.9508, 0.9511, 0.9519, **Epoch: 80
0.9531, 0.9511, 0.9522, 0.9541, 0.9515, 0.9530, 0.9531, 0.9531, 0.9509, 0.9534, **Epoch: 90
0.9560, 0.9516, 0.9508, 0.9565, 0.9534, 0.9545, 0.9509, 0.9521, 0.9528, 0.9550, **Epoch: 100
Validation AUC Final: 0.9550
Learning rate: 0.01
Weight decay: 0.0005
Drop rate: 0
Epochs: 100
--------------------------END------------------------------
